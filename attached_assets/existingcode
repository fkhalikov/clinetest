import requests
import pandas as pd
import time
import random
import logging

# Set up logging: output to file and console
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("scraping.log", mode="w"),
        logging.StreamHandler()
    ]
)

# New AJAX endpoint for fund search
base_url = "https://www.hl.co.uk/ajax/funds/fund-search/search"

# Query parameters for the request
params = {
    "investment": "",
    "companyid": "",
    "sectorid": "",
    "wealth": "",
    "unitTypePref": "",
    "tracker": "",
    "payment_frequency": "",
    "payment_type": "",
    "yield": "",
    "standard_ocf": "",
    "perf12m": "",
    "perf36m": "",
    "perf60m": "",
    "fund_size": "",
    "num_holdings": "",
    "start": 0,
    "rpp": 20,
    "lo": 0,
    "sort": "fd.full_description",
    "sort_dir": "asc"
}

# List of sample User-Agent strings to rotate
user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 12_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15",
    "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:102.0) Gecko/20100101 Firefox/102.0",
]

funds_data = []
page = 0
total_results = None

while total_results is None or page * params["rpp"] < total_results:
    # Randomize the User-Agent header for each request
    headers = {"User-Agent": random.choice(user_agents)}
    params["start"] = page * params["rpp"]
    
    logging.info(f"Requesting page {page+1} - URL: {base_url} with params: {params}")
    try:
        response = requests.get(base_url, headers=headers, params=params, timeout=10)
    except Exception as e:
        logging.exception("Request failed:")
        break

    logging.info(f"Status Code: {response.status_code}")
    
    if response.status_code != 200:
        logging.error(f"Error: Received status code {response.status_code}")
        break

    snippet = response.text[:200]
    logging.debug("Response snippet: %s", snippet)
    
    try:
        json_data = response.json()
    except Exception as e:
        logging.exception("Failed to decode JSON. Full response:")
        logging.debug(response.text)
        break

    if total_results is None:
        total_results = json_data.get("TotalResults", 0)
        logging.info(f"Total results: {total_results}")
    
    for fund in json_data.get("Results", []):
        funds_data.append({
            "SEDOL": fund.get("sedol", ""),
            "Name": fund.get("fund_name", ""),
            "Company": fund.get("company_name", ""),
            "Annual Charge (%)": fund.get("annual_charge", ""),
            "Fund Size (M)": fund.get("fund_size", ""),
            "1Y Performance (%)": fund.get("perf12m", ""),
            "Yield (%)": fund.get("yield", ""),
            "Bid Price": fund.get("bid_price", ""),
            "Offer Price": fund.get("offer_price", ""),
            "KIID URL": fund.get("kiid_url", ""),
            "Updated": fund.get("updated", ""),
        })

    logging.info(f"Scraped {len(funds_data)} funds so far...")
    page += 1
    # Sleep for a random time between 1 and 3 seconds to reduce request frequency
    delay = random.uniform(1, 3)
    logging.debug(f"Sleeping for {delay:.2f} seconds")
    time.sleep(delay)

if funds_data:
    df = pd.DataFrame(funds_data)
    df.to_csv("hl_funds.csv", index=False)
    logging.info("Scraping complete! Data saved to hl_funds.csv")
else:
    logging.warning("No fund data was scraped.")
